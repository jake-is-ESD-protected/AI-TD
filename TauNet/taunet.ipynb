{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.12.0\n"
     ]
    }
   ],
   "source": [
    "# file related\n",
    "import os\n",
    "from os.path import join\n",
    "import datetime\n",
    "\n",
    "# machine learning\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import Audio as play_audio\n",
    "import scipy as sp\n",
    "import taunet_utils\n",
    "import json\n",
    "\n",
    "# RTNeural special import\n",
    "import sys\n",
    "sys.path.append(\"../RTNeural/python/\")\n",
    "from model_utils import save_model\n",
    "\n",
    "print(f\"TF version {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: [0.7748, 0.012, 0.2251, 0.1621, 0.1335, 0.4836, 0.5895, 0.1336, 0.0194, 0.1007, 0.0475, 0.6738, 0.8758] -> [0.7210246, 0.3815809]\n",
      "2: [0.9566, 0.0322, 0.1695, 0.111, 0.1284, 0.6706, 0.2561, 0.0526, 0.035, 0.0238, 0.0945, 0.7384, 0.2437] -> [0.9999503999999999, 0.1248695]\n",
      "3: [0.5347, 0.0236, 0.3093, 0.0635, 0.1078, 0.5009, 0.3548, 0.1477, 0.0242, 0.049, 0.0797, 0.2224, 0.9991] -> [0.0452408, 0.6440535333333333]\n",
      "4: [0.567, 0.0294, 0.312, 0.0813, 0.1557, 0.639, 0.6673, 0.2165, 0.0254, 0.0805, 0.0533, 0.6711, 0.8347] -> [0.2988928, 0.4453225]\n",
      "5: [0.6793, 0.012, 0.2543, 0.1456, 0.0491, 0.6359, 0.2842, 0.046, 0.0249, 0.055, 0.1009, 0.6149, 0.8181] -> [0.7027057999999999, 0.5654544666666667]\n",
      "6: [0.543, 0.0087, 0.1529, 0.2627, 0.0063, 0.0794, 0.1626, 0.2719, 0.2605, 0.0991, 0.189, 0.2, 0.9179] -> [0.1832008, 0.8069385333333333]\n",
      "7: [0.5835, 0.0172, 0.3052, 0.1932, 0.0524, 0.5275, 0.5217, 0.1129, 0.043, 0.0948, 0.0886, 0.6255, 0.4053] -> [0.9999752, 0.07385166666666666]\n",
      "8: [0.6222, 0.0269, 0.1893, 0.0367, 0.1135, 0.6748, 0.2956, 0.0856, 0.0237, 0.032, 0.0254, 0.6357, 0.3034] -> [0.9407981999999999, 0.07256796666666666]\n",
      "9: [0.748, 0.0604, 0.4037, 0.0353, 0.0748, 0.2203, 0.5865, 0.2944, 0.0326, 0.0484, 0.0314, 0.6341, 0.7424] -> [0.4936854, 0.5957196]\n",
      "10: [0.9938, 0.0131, 0.2123, 0.0253, 0.0839, 0.2592, 0.531, 0.4017, 0.0325, 0.032, 0.0459, 0.3157, 0.8187] -> [0.661116, 0.6049652666666667]\n",
      "11: [0.7363, 0.1348, 0.5111, 0.0849, 0.0891, 0.1182, 0.4665, 0.2158, 0.0346, 0.0606, 0.0382, 0.7365, 0.7567] -> [0.6050882000000001, 0.1435178]\n",
      "12: [0.5388, 0.3645, 0.4617, 0.1641, 0.0347, 0.0511, 0.5516, 0.588, 0.0933, 0.079, 0.0358, 0.7287, 0.8574] -> [0.1403228, 0.39766619999999997]\n",
      "13: [0.98, 0.0153, 0.3311, 0.0331, 0.1091, 0.7279, 0.3735, 0.1466, 0.018, 0.0352, 0.0392, 0.109, 0.9997] -> [0.5679371999999999, 0.5237308666666667]\n",
      "14: [0.9534, 0.0422, 0.245, 0.0219, 0.1035, 0.2516, 0.4322, 0.322, 0.0148, 0.0352, 0.019, 0.6313, 0.2773] -> [0.5639571999999999, 0.15391586666666665]\n",
      "15: [0.8986, 1.4763, 0.1339, 0.0222, 0.0999, 0.2547, 0.5552, 0.4316, 0.0236, 0.0492, 0.0047, 0.5662, 0.8151] -> [0.413524, 0.5849622333333333]\n",
      "16: [0.799, 0.0328, 0.2358, 0.0221, 0.1786, 0.3017, 0.4926, 0.1176, 0.0092, 0.0373, 0.036, 0.6062, 0.2058] -> [0.9992584000000001, 0.10698596666666667]\n",
      "17: [0.8762, 0.0419, 0.1801, 0.0961, 0.1867, 0.8984, 0.2507, 0.0235, 0.0128, 0.0234, 0.0272, 0.2384, 0.8137] -> [0.1449692, 0.35758799999999996]\n",
      "18: [0.5716, 0.0413, 0.1093, 0.0854, 0.1076, 0.7635, 0.2227, 0.0715, 0.0218, 0.0323, 0.0522, 0.9379, 0.7991] -> [0.4766844, 0.08788873333333333]\n",
      "19: [0.5022, 0.0558, 0.1917, 0.1134, 0.1422, 0.1815, 0.2041, 0.0592, 0.0205, 0.0364, 0.0719, 0.1, 0.9998] -> [0.5147938000000001, 0.6016464333333333]\n",
      "20: [0.5909, 0.1661, 0.4584, 0.0579, 0.0203, 0.8714, 0.6452, 0.5185, 0.0881, 0.0486, 0.0621, 0.9252, 0.0] -> [0.22442240000000002, 0.21709183333333332]\n",
      "21: [0.9375, 0.0149, 0.1608, 0.1273, 0.1286, 0.6967, 0.1587, 0.1363, 0.038, 0.0347, 0.0675, 0.5384, 0.8216] -> [0.858212, 0.14131153333333332]\n",
      "22: [0.9191, 0.0252, 0.1561, 0.0266, 0.5622, 0.5345, 0.2129, 0.0394, 0.0044, 0.0412, 0.0848, 0.5007, 0.2] -> [0.5429486, 0.32457626666666667]\n",
      "23: [0.9375, 0.0574, 0.1928, 0.0913, 0.1374, 0.4825, 0.2248, 0.1055, 0.0255, 0.0302, 0.1542, 0.6167, 0.2] -> [0.8428538, 0.4218914333333333]\n",
      "24: [0.9375, 0.0188, 0.1594, 0.1346, 0.0283, 0.1579, 0.3294, 0.1234, 0.0636, 0.0557, 0.0535, 0.5016, 0.8097] -> [0.14358420000000002, 0.11016763333333333]\n",
      "25: [0.8762, 0.0539, 0.2009, 0.0798, 0.1095, 0.3662, 0.1703, 0.0648, 0.0324, 0.0354, 0.0479, 0.4924, 0.8547] -> [0.3051044, 0.7448993333333332]\n",
      "26: [0.5493, 0.0296, 0.1577, 0.0788, 0.072, 0.5483, 0.3456, 0.1064, 0.0347, 0.0318, 0.0835, 0.7254, 0.55] -> [0.9453346, 0.14960826666666666]\n",
      "27: [0.5493, 0.0319, 0.2719, 0.0638, 0.046, 0.5718, 0.6515, 0.3671, 0.0525, 0.0461, 0.1555, 0.2685, 0.7946] -> [0.60561, 0.14421953333333332]\n",
      "28: [0.6893, 0.0185, 0.2294, 0.0787, 0.0744, 0.3627, 0.3756, 0.1513, 0.0276, 0.0519, 0.113, 0.2503, 0.81] -> [0.6056502, 0.11944516666666667]\n",
      "29: [0.75, 0.0467, 0.4485, 0.2283, 0.0885, 0.2593, 0.3034, 0.1781, 0.0331, 0.069, 0.0189, 0.2941, 0.9105] -> [0.1174944, 0.8291477666666667]\n",
      "30: [0.754, 0.0323, 0.2839, 0.1234, 0.035, 0.6594, 0.2727, 0.1743, 0.0982, 0.0527, 0.0246, 0.5815, 0.1677] -> [0.7436024, 0.13427113333333332]\n",
      "31: [0.5208, 0.0225, 0.1591, 0.0632, 0.2304, 0.5337, 0.1942, 0.1242, 0.0151, 0.0409, 0.0107, 0.5916, 0.324] -> [0.9999752, 0.5807066]\n",
      "32: [0.9566, 0.0201, 0.2427, 0.119, 0.1426, 0.4572, 0.4189, 0.1178, 0.0377, 0.0424, 0.0256, 0.3395, 0.766] -> [0.9993572000000001, 0.5614687333333332]\n",
      "33: [0.7212, 0.0536, 0.1896, 0.1346, 0.0772, 0.5218, 0.1995, 0.2324, 0.0546, 0.0485, 0.0925, 0.6804, 0.2354] -> [0.6777312, 0.7317597333333333]\n",
      "34: [0.9502, 0.0109, 0.0955, 0.0692, 0.122, 0.2714, 0.3586, 0.1826, 0.0136, 0.0407, 0.06, 0.6663, 0.9598] -> [0.26921080000000003, 0.24413763333333335]\n",
      "35: [0.7945, 0.0326, 0.2023, 0.1039, 0.096, 0.3754, 0.191, 0.1539, 0.0327, 0.0406, 0.0089, 0.79, 0.7839] -> [0.8717812, 0.6440764666666666]\n",
      "36: [0.8762, 0.0391, 0.2229, 0.1058, 0.0498, 0.7281, 0.294, 0.1817, 0.044, 0.0382, 0.1123, 0.2839, 0.7826] -> [0.29398779999999997, 0.5025583]\n",
      "37: [0.5307, 0.0255, 0.172, 0.1405, 0.1629, 0.4232, 0.1879, 0.1082, 0.0199, 0.0431, 0.0086, 0.2839, 0.2469] -> [0.29424, 0.13091183333333334]\n",
      "38: [0.7267, 0.0121, 0.2253, 0.0289, 0.1842, 0.5535, 0.2077, 0.0789, 0.0216, 0.0366, 0.0517, 0.9343, 0.2477] -> [0.44649419999999995, 0.2931283333333333]\n",
      "39: [0.8446, 0.0378, 0.1944, 0.0671, 0.2083, 0.6049, 0.2186, 0.0971, 0.0262, 0.0322, 0.0839, 0.7667, 0.9138] -> [0.203009, 0.2766109]\n",
      "40: [0.7175, 0.0193, 0.1472, 0.1111, 0.0804, 0.2148, 0.3105, 0.1075, 0.0342, 0.0305, 0.1712, 0.1961, 0.9731] -> [0.1055884, 0.2708006666666667]\n",
      "41: [0.8681, 0.013, 0.1897, 0.1427, 0.112, 0.0732, 0.2151, 0.2022, 0.0347, 0.0603, 0.0922, 0.8142, 0.8705] -> [0.4439738, 0.76886]\n",
      "42: [0.5763, 0.0133, 0.1206, 0.1311, 0.1981, 0.8414, 0.1276, 0.061, 0.0183, 0.0385, 0.0255, 0.5115, 0.9999] -> [0.0876308, 0.20916336666666666]\n",
      "43: [0.5409, 0.0457, 0.2016, 0.0566, 0.2403, 0.6282, 0.1625, 0.0978, 0.0117, 0.0421, 0.0222, 0.2125, 0.9111] -> [0.44994080000000003, 0.10185939999999999]\n",
      "44: [0.5716, 0.0429, 0.1394, 0.0813, 0.1471, 0.7489, 0.345, 0.0717, 0.0169, 0.0367, 0.1543, 0.6257, 0.9111] -> [0.1063182, 0.199156]\n",
      "45: [0.7945, 0.064, 0.1966, 0.0701, 0.1777, 0.4989, 0.2489, 0.109, 0.0211, 0.0369, 0.0296, 0.6312, 0.3742] -> [0.5148876, 0.05010836666666666]\n",
      "46: [0.744, 0.0214, 0.1387, 0.0929, 0.1201, 0.4846, 0.1782, 0.0627, 0.0187, 0.0331, 0.0972, 0.1037, 0.3663] -> [0.6310621999999999, 0.07326586666666667]\n",
      "47: [0.9938, 0.0059, 0.2267, 0.0388, 0.1443, 0.8726, 0.2879, 0.1955, 0.0322, 0.0305, 0.0254, 0.1048, 0.7492] -> [0.6334944, 0.2870798]\n",
      "48: [0.5884, 0.0394, 0.1795, 0.0908, 0.0788, 0.4641, 0.2636, 0.2834, 0.0338, 0.053, 0.0458, 0.4944, 0.9] -> [0.6336786, 0.17670513333333335]\n",
      "49: [0.8762, 0.0296, 0.5174, 0.0842, 0.1432, 0.5428, 0.3229, 0.1187, 0.0304, 0.053, 0.0801, 0.2449, 0.9615] -> [0.634878, 0.21476550000000003]\n",
      "50: [0.5228, 0.0202, 0.1536, 0.1063, 0.1187, 0.7092, 0.3751, 0.0751, 0.0196, 0.0328, 0.0861, 0.6273, 0.8602] -> [0.23124940000000002, 0.34019143333333335]\n",
      "51: [0.5004, 0.0215, 0.1547, 0.0787, 0.0574, 0.5012, 0.3639, 0.1878, 0.0369, 0.0337, 0.1884, 0.1535, 0.2204] -> [0.6709282, 0.15848113333333333]\n",
      "52: [0.5004, 0.0107, 0.1133, 0.0268, 0.2833, 0.4258, 0.293, 0.0516, 0.0058, 0.0313, 0.0994, 0.2779, 0.731] -> [0.6978852000000001, 0.23806543333333333]\n",
      "53: [0.8627, 0.0281, 0.1156, 0.1514, 0.1512, 0.4468, 0.1265, 0.078, 0.0182, 0.042, 0.0691, 0.2809, 0.7829] -> [0.11961820000000001, 0.5914650666666667]\n",
      "54: [0.5189, 0.024, 0.1076, 0.1125, 0.0518, 0.6913, 0.3551, 0.0646, 0.0201, 0.031, 0.0948, 0.6411, 0.7838] -> [0.0715436, 0.5925381]\n",
      "55: [0.7684, 0.0216, 0.1274, 0.0758, 0.122, 0.4006, 0.2748, 0.1318, 0.0168, 0.0323, 0.0722, 0.5695, 0.7159] -> [0.9999752, 0.999971]\n",
      "56: [0.752, 0.0214, 0.1174, 0.0686, 0.1428, 0.3257, 0.3052, 0.1122, 0.0194, 0.0356, 0.1153, 0.27, 0.7147] -> [0.305853, 0.9995362]\n",
      "57: [0.8446, 0.0232, 0.1209, 0.1086, 0.1453, 0.2462, 0.307, 0.1283, 0.0306, 0.0423, 0.0756, 0.287, 0.8795] -> [0.737533, 0.1364524]\n",
      "58: [0.6826, 0.5251, 0.7008, 0.1174, 0.1029, 0.2575, 0.1431, 0.466, 0.0256, 0.0615, 0.0038, 0.2181, 0.8451] -> [0.1106064, 0.29515653333333336]\n",
      "59: [0.9665, 0.0136, 0.1331, 0.0157, 0.2166, 0.2862, 0.4967, 0.1057, 0.0076, 0.0307, 0.02, 0.569, 0.7956] -> [0.307788, 0.14481393333333334]\n"
     ]
    }
   ],
   "source": [
    "with open(join(\"dataset\", \"saved\", \"AITD_Dataset_Kristof_beta_1in.json\"), \"r\") as json_file:\n",
    "    input_data = json.load(json_file)\n",
    "with open(join(\"dataset\", \"saved\", \"AITD_Dataset_Kristof_beta_1out.json\"), \"r\") as json_file:\n",
    "    output_data = json.load(json_file)\n",
    "\n",
    "for d in input_data:\n",
    "    print(f\"{d}: {input_data[d]} -> {output_data[d]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_7 (Dense)             (None, 64)                896       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 2)                 10        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,742\n",
      "Trainable params: 4,742\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(4, activation='relu'))\n",
    "    # model.add(layers.LSTM(64, return_sequences=True))   # short term memory, useful if input data is related accross vectors\n",
    "    model.add(Dense(2, kernel_regularizer=tf.keras.regularizers.l2(0.001))) # no activation (linear): continuous mapping of outputs (this is not a classification task!)\n",
    "    return model\n",
    "\n",
    "input_shape = (13,)\n",
    "model = create_model(input_shape)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 11700), started 9 days, 21:47:09 ago. (Use '!kill 11700' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5c013104d9c8a487\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5c013104d9c8a487\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2/2 [==============================] - 5s 878ms/step - loss: 0.2589 - accuracy: 0.5682 - val_loss: 0.2084 - val_accuracy: 0.6667\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 0s 375ms/step - loss: 0.2359 - accuracy: 0.5682 - val_loss: 0.1860 - val_accuracy: 0.6667\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 0s 311ms/step - loss: 0.2114 - accuracy: 0.6136 - val_loss: 0.1604 - val_accuracy: 0.6667\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 0s 302ms/step - loss: 0.1878 - accuracy: 0.6136 - val_loss: 0.1362 - val_accuracy: 0.6667\n",
      "Epoch 5/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1601 - accuracy: 0.6562WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0031s vs `on_train_batch_end` time: 0.0061s). Check your callbacks.\n",
      "2/2 [==============================] - 0s 340ms/step - loss: 0.1633 - accuracy: 0.6136 - val_loss: 0.1145 - val_accuracy: 0.6667\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 0s 352ms/step - loss: 0.1390 - accuracy: 0.6136 - val_loss: 0.0986 - val_accuracy: 0.6667\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 0s 371ms/step - loss: 0.1283 - accuracy: 0.6136 - val_loss: 0.0932 - val_accuracy: 0.6667\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 0s 376ms/step - loss: 0.1210 - accuracy: 0.6136 - val_loss: 0.0938 - val_accuracy: 0.6667\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 0s 308ms/step - loss: 0.1200 - accuracy: 0.6136 - val_loss: 0.0953 - val_accuracy: 0.6667\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 0s 316ms/step - loss: 0.1213 - accuracy: 0.6136 - val_loss: 0.0950 - val_accuracy: 0.6667\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 0s 316ms/step - loss: 0.1208 - accuracy: 0.6136 - val_loss: 0.0923 - val_accuracy: 0.6667\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 0s 410ms/step - loss: 0.1176 - accuracy: 0.6136 - val_loss: 0.0883 - val_accuracy: 0.6667\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 0s 327ms/step - loss: 0.1136 - accuracy: 0.6136 - val_loss: 0.0849 - val_accuracy: 0.6667\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 0s 351ms/step - loss: 0.1098 - accuracy: 0.6136 - val_loss: 0.0830 - val_accuracy: 0.6667\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 0s 412ms/step - loss: 0.1082 - accuracy: 0.6136 - val_loss: 0.0824 - val_accuracy: 0.6667\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 0s 345ms/step - loss: 0.1075 - accuracy: 0.6136 - val_loss: 0.0823 - val_accuracy: 0.6667\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 0s 348ms/step - loss: 0.1066 - accuracy: 0.6136 - val_loss: 0.0820 - val_accuracy: 0.6667\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 0s 348ms/step - loss: 0.1055 - accuracy: 0.6136 - val_loss: 0.0814 - val_accuracy: 0.6667\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 0s 489ms/step - loss: 0.1040 - accuracy: 0.6136 - val_loss: 0.0802 - val_accuracy: 0.6667\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 0s 439ms/step - loss: 0.1022 - accuracy: 0.6136 - val_loss: 0.0787 - val_accuracy: 0.6667\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 0s 413ms/step - loss: 0.1003 - accuracy: 0.6136 - val_loss: 0.0774 - val_accuracy: 0.6667\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 1s 556ms/step - loss: 0.0986 - accuracy: 0.6136 - val_loss: 0.0766 - val_accuracy: 0.6667\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 0s 438ms/step - loss: 0.0969 - accuracy: 0.6136 - val_loss: 0.0760 - val_accuracy: 0.6667\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 0s 369ms/step - loss: 0.0958 - accuracy: 0.6136 - val_loss: 0.0755 - val_accuracy: 0.6667\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 0s 408ms/step - loss: 0.0952 - accuracy: 0.6136 - val_loss: 0.0750 - val_accuracy: 0.6667\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 0s 300ms/step - loss: 0.0937 - accuracy: 0.6136 - val_loss: 0.0741 - val_accuracy: 0.6667\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 0s 313ms/step - loss: 0.0926 - accuracy: 0.6136 - val_loss: 0.0733 - val_accuracy: 0.6667\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 0s 317ms/step - loss: 0.0907 - accuracy: 0.6136 - val_loss: 0.0730 - val_accuracy: 0.6667\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 0s 401ms/step - loss: 0.0899 - accuracy: 0.6136 - val_loss: 0.0727 - val_accuracy: 0.6667\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 0s 349ms/step - loss: 0.0881 - accuracy: 0.6136 - val_loss: 0.0725 - val_accuracy: 0.6667\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 0s 306ms/step - loss: 0.0870 - accuracy: 0.6136 - val_loss: 0.0724 - val_accuracy: 0.6667\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 0s 302ms/step - loss: 0.0859 - accuracy: 0.6136 - val_loss: 0.0722 - val_accuracy: 0.6667\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 0s 356ms/step - loss: 0.0851 - accuracy: 0.6136 - val_loss: 0.0719 - val_accuracy: 0.6667\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 0s 379ms/step - loss: 0.0840 - accuracy: 0.6136 - val_loss: 0.0709 - val_accuracy: 0.6667\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 0s 340ms/step - loss: 0.0825 - accuracy: 0.6136 - val_loss: 0.0698 - val_accuracy: 0.6667\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 303ms/step - loss: 0.0817 - accuracy: 0.6136 - val_loss: 0.0694 - val_accuracy: 0.6667\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 0s 470ms/step - loss: 0.0825 - accuracy: 0.6136 - val_loss: 0.0694 - val_accuracy: 0.6667\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 0s 435ms/step - loss: 0.0816 - accuracy: 0.6136 - val_loss: 0.0685 - val_accuracy: 0.6667\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 0s 375ms/step - loss: 0.0801 - accuracy: 0.6136 - val_loss: 0.0680 - val_accuracy: 0.6667\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 1s 522ms/step - loss: 0.0790 - accuracy: 0.6136 - val_loss: 0.0679 - val_accuracy: 0.6667\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 0s 376ms/step - loss: 0.0787 - accuracy: 0.6136 - val_loss: 0.0682 - val_accuracy: 0.6667\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 0s 366ms/step - loss: 0.0776 - accuracy: 0.6136 - val_loss: 0.0684 - val_accuracy: 0.6667\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 349ms/step - loss: 0.0767 - accuracy: 0.6136 - val_loss: 0.0690 - val_accuracy: 0.6667\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 0s 341ms/step - loss: 0.0769 - accuracy: 0.6136 - val_loss: 0.0699 - val_accuracy: 0.6667\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 422ms/step - loss: 0.0768 - accuracy: 0.6136 - val_loss: 0.0693 - val_accuracy: 0.6667\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 403ms/step - loss: 0.0758 - accuracy: 0.6136 - val_loss: 0.0680 - val_accuracy: 0.6667\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 0s 334ms/step - loss: 0.0744 - accuracy: 0.6136 - val_loss: 0.0672 - val_accuracy: 0.6667\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 0s 361ms/step - loss: 0.0742 - accuracy: 0.6136 - val_loss: 0.0669 - val_accuracy: 0.6667\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 394ms/step - loss: 0.0741 - accuracy: 0.6136 - val_loss: 0.0667 - val_accuracy: 0.6667\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 0s 357ms/step - loss: 0.0738 - accuracy: 0.6136 - val_loss: 0.0660 - val_accuracy: 0.6667\n"
     ]
    }
   ],
   "source": [
    "combined_data = [(input_data[key], output_data[key]) for key in input_data.keys()]\n",
    "\n",
    "train_data, test_data = train_test_split(combined_data, test_size=0.05, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "log_dir = \"logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "class LogRMSECallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, log_dir):\n",
    "        super(LogRMSECallback, self).__init__()\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        mse = logs.get('val_loss')\n",
    "        rmse = np.sqrt(mse)\n",
    "        with tf.summary.create_file_writer(self.log_dir).as_default():\n",
    "            tf.summary.scalar('val_rmse', rmse, step=epoch)\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/ --port 6006\n",
    "\n",
    "history = model.fit(\n",
    "    x=np.array([item[0] for item in train_data]),\n",
    "    y=np.array([item[1] for item in train_data]),\n",
    "    validation_data=(\n",
    "        np.array([item[0] for item in val_data]),\n",
    "        np.array([item[1] for item in val_data])\n",
    "    ),\n",
    "    epochs=50,\n",
    "    # callbacks=[tensorboard_callback, early_stopping_callback],\n",
    "    callbacks=[tensorboard_callback, early_stopping_callback, LogRMSECallback(join(log_dir, \"rmse\"))],\n",
    "    # batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 241ms/step\n",
      "Input: [0.7748, 0.012, 0.2251, 0.1621, 0.1335, 0.4836, 0.5895, 0.1336, 0.0194, 0.1007, 0.0475, 0.6738, 0.8758]\n",
      "Expected Output: [0.7210246, 0.3815809]\n",
      "Predicted Output: [[0.5249382  0.24885817]]\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "Input: [0.543, 0.0087, 0.1529, 0.2627, 0.0063, 0.0794, 0.1626, 0.2719, 0.2605, 0.0991, 0.189, 0.2, 0.9179]\n",
      "Expected Output: [0.1832008, 0.8069385333333333]\n",
      "Predicted Output: [[0.3898979  0.20542346]]\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "Input: [0.7945, 0.0326, 0.2023, 0.1039, 0.096, 0.3754, 0.191, 0.1539, 0.0327, 0.0406, 0.0089, 0.79, 0.7839]\n",
      "Expected Output: [0.8717812, 0.6440764666666666]\n",
      "Predicted Output: [[0.54984206 0.2568683 ]]\n",
      "{'Train loss': 0.07281644642353058, 'Train accuracy': 0.6136363744735718}\n",
      "{'Validation loss': 0.06596104800701141, 'Validation accuracy': 0.6666666865348816}\n",
      "{'Test loss': 0.12247678637504578, 'Test accuracy': 0.6666666865348816}\n",
      "RMSE for test set: 0.34996683610743146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/20240505-133556\\taunet.pb\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/20240505-133556\\taunet.pb\\assets\n"
     ]
    }
   ],
   "source": [
    "train_metrics = model.evaluate(\n",
    "    np.array([item[0] for item in train_data]),\n",
    "    np.array([item[1] for item in train_data]),\n",
    "    verbose=0\n",
    ")\n",
    "val_metrics = model.evaluate(\n",
    "    np.array([item[0] for item in val_data]),\n",
    "    np.array([item[1] for item in val_data]),\n",
    "    verbose=0\n",
    ")\n",
    "test_metrics = model.evaluate(\n",
    "    np.array([item[0] for item in test_data]),\n",
    "    np.array([item[1] for item in test_data]),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "for item in test_data:\n",
    "    inp = np.array([item[0]])  # Reshape input data into a batch\n",
    "    prediction = model.predict(inp)\n",
    "    print(\"Input:\", item[0])\n",
    "    print(\"Expected Output:\", item[1])\n",
    "    print(\"Predicted Output:\", prediction)\n",
    "\n",
    "\n",
    "print({f\"Train {metric_name}\": metric_value for metric_name, metric_value in zip(model.metrics_names, train_metrics)})\n",
    "print({f\"Validation {metric_name}\": metric_value for metric_name, metric_value in zip(model.metrics_names, val_metrics)})\n",
    "print({f\"Test {metric_name}\": metric_value for metric_name, metric_value in zip(model.metrics_names, test_metrics)})\n",
    "print(\"RMSE for test set:\", np.sqrt(test_metrics[0]))\n",
    "\n",
    "# save model as .h5, .tflite and .json\n",
    "model.save(join(log_dir, \"taunet.pb\"))\n",
    "tflite_model = tf.lite.TFLiteConverter.from_saved_model(join(log_dir, \"taunet.pb\")).convert()\n",
    "with open(join(log_dir, \"taunet.tflite\"), 'wb') as f:\n",
    "  f.write(tflite_model)\n",
    "save_model(model, join(log_dir, \"taunet.json\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
