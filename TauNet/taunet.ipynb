{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.12.0\n"
     ]
    }
   ],
   "source": [
    "# file related\n",
    "import os\n",
    "from os.path import join\n",
    "import datetime\n",
    "\n",
    "# machine learning\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import Audio as play_audio\n",
    "import scipy as sp\n",
    "import taunet_utils\n",
    "import json\n",
    "\n",
    "# RTNeural special import\n",
    "import sys\n",
    "sys.path.append(\"../RTNeural/python/\")\n",
    "from model_utils import save_model\n",
    "\n",
    "print(f\"TF version {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: [123.9669, 0.012, 0.2251, 15648, 0.9176, 82, 10, 5, 1, 47.5291, 0.6738, 0.8758, 550.0, 350.0] -> [360.5123, 1144.7427]\n",
      "2: [153.0612, 0.0322, 0.1695, 10716, 0.8286, 137, 7, 3, 2, 94.4725, 0.7384, 0.2437, 550.0, 350.0] -> [499.9752, 374.6085]\n",
      "3: [171.1027, 0.0236, 0.3093, 6128, 0.807, 72, 7, 5, 1, 79.7098, 0.2224, 0.9991, 550.0, 350.0] -> [22.6204, 1932.1606]\n",
      "4: [181.4516, 0.0294, 0.312, 7848, 0.6281, 82, 9, 5, 1, 53.2918, 0.6711, 0.8347, 550.0, 350.0] -> [149.4464, 1335.9675]\n",
      "5: [108.6957, 0.012, 0.2543, 14056, 0.788, 110, 6, 3, 2, 100.8992, 0.6149, 0.8181, 550.0, 350.0] -> [351.3529, 1696.3634]\n",
      "6: [173.7452, 0.0087, 0.1529, 25353, 0.6351, 3, 3, 4, 3, 188.9978, 0.2, 0.9179, 550.0, 350.0] -> [91.6004, 2420.8156]\n",
      "7: [186.722, 0.0172, 0.3052, 18645, 0.8797, 67, 8, 3, 2, 88.6245, 0.6255, 0.4053, 550.0, 350.0] -> [499.9876, 221.555]\n",
      "8: [199.115, 0.0269, 0.1893, 3541, 1.1604, 97, 6, 3, 1, 25.3582, 0.6357, 0.3034, 550.0, 350.0] -> [470.3991, 217.7039]\n",
      "9: [119.6809, 0.0604, 0.4037, 3407, 1.3411, 22, 6, 5, 1, 31.3722, 0.6341, 0.7424, 550.0, 350.0] -> [246.8427, 1787.1588]\n",
      "10: [159.0106, 0.0131, 0.2123, 2440, 0.9433, 27, 7, 7, 1, 45.9326, 0.3157, 0.8187, 550.0, 350.0] -> [330.558, 1814.8958]\n",
      "11: [117.801, 0.1348, 0.5111, 8194, 0.9682, 11, 7, 4, 1, 38.2294, 0.7365, 0.7567, 550.0, 350.0] -> [302.5441, 430.5534]\n",
      "12: [172.4138, 0.3645, 0.4617, 15839, 1.569, 1, 3, 3, 2, 35.7993, 0.7287, 0.8574, 550.0, 350.0] -> [70.1614, 1192.9986]\n",
      "13: [156.7944, 0.0153, 0.3311, 3190, 0.8861, 88, 6, 4, 1, 39.177, 0.109, 0.9997, 550.0, 350.0] -> [283.9686, 1571.1926]\n",
      "14: [152.5424, 0.0422, 0.245, 2114, 1.2588, 24, 6, 6, 1, 18.9737, 0.6313, 0.2773, 550.0, 350.0] -> [281.9786, 461.7476]\n",
      "15: [143.77, 1.4763, 0.1339, 2141, 2.2479, 13, 5, 7, 1, 4.7146, 0.5662, 0.8151, 550.0, 350.0] -> [206.762, 1754.8867]\n",
      "16: [127.8409, 0.0328, 0.2358, 2134, 0.8774, 39, 8, 4, 1, 36.0141, 0.6062, 0.2058, 550.0, 350.0] -> [499.6292, 320.9579]\n",
      "17: [140.1869, 0.0419, 0.1801, 9275, 1.0397, 186, 7, 2, 1, 27.1993, 0.2384, 0.8137, 550.0, 350.0] -> [72.4846, 1072.764]\n",
      "18: [182.9268, 0.0413, 0.1093, 8239, 0.845, 141, 6, 3, 2, 52.2142, 0.9379, 0.7991, 550.0, 350.0] -> [238.3422, 263.6662]\n",
      "19: [160.7143, 0.0558, 0.1917, 10943, 1.0817, 35, 6, 3, 2, 71.935, 0.1, 0.9998, 550.0, 350.0] -> [257.3969, 1804.9393]\n",
      "20: [189.0756, 0.1661, 0.4584, 5591, 0.7392, 33, 4, 4, 1, 62.0841, 0.9252, 0.0, 550.0, 350.0] -> [112.2112, 651.2755]\n",
      "21: [150.0, 0.0149, 0.1608, 12284, 0.8486, 123, 5, 5, 2, 67.5176, 0.5384, 0.8216, 550.0, 350.0] -> [429.106, 423.9346]\n",
      "22: [147.0588, 0.0252, 0.1561, 2562, 1.3738, 103, 6, 2, 0, 84.7788, 0.5007, 0.2, 550.0, 350.0] -> [271.4743, 973.7288]\n",
      "23: [150.0, 0.0574, 0.1928, 8812, 0.6881, 92, 6, 5, 2, 154.2055, 0.6167, 0.2, 550.0, 350.0] -> [421.4269, 1265.6743]\n",
      "24: [150.0, 0.0188, 0.1594, 12988, 0.8422, 11, 4, 3, 2, 53.5006, 0.5016, 0.8097, 550.0, 350.0] -> [71.7921, 330.5029]\n",
      "25: [140.1869, 0.0539, 0.2009, 7704, 0.8947, 55, 5, 3, 2, 47.8611, 0.4924, 0.8547, 550.0, 350.0] -> [152.5522, 2234.698]\n",
      "26: [175.7812, 0.0296, 0.1577, 7608, 0.6711, 94, 7, 4, 2, 83.5337, 0.7254, 0.55, 550.0, 350.0] -> [472.6673, 448.8248]\n",
      "27: [175.7812, 0.0319, 0.2719, 6159, 0.6361, 61, 8, 6, 2, 155.512, 0.2685, 0.7946, 550.0, 350.0] -> [302.805, 432.6586]\n",
      "28: [110.2941, 0.0185, 0.2294, 7597, 1.0254, 38, 6, 4, 1, 112.9753, 0.2503, 0.81, 550.0, 350.0] -> [302.8251, 358.3355]\n",
      "29: [120.0, 0.0467, 0.4485, 22029, 1.5532, 25, 5, 4, 1, 18.9379, 0.2941, 0.9105, 550.0, 350.0] -> [58.7472, 2487.4433]\n",
      "30: [120.6434, 0.0323, 0.2839, 11913, 0.7958, 53, 5, 3, 2, 24.579, 0.5815, 0.1677, 550.0, 350.0] -> [371.8012, 402.8134]\n",
      "31: [166.6667, 0.0225, 0.1591, 6098, 0.5236, 100, 6, 5, 1, 10.6641, 0.5916, 0.324, 550.0, 350.0] -> [499.9876, 1742.1198]\n",
      "32: [153.0612, 0.0201, 0.2427, 11483, 0.6263, 70, 7, 4, 2, 25.6205, 0.3395, 0.766, 550.0, 350.0] -> [499.6786, 1684.4062]\n",
      "33: [115.3846, 0.0536, 0.1896, 12994, 0.7246, 65, 5, 5, 2, 92.4555, 0.6804, 0.2354, 550.0, 350.0] -> [338.8656, 2195.2792]\n",
      "34: [152.027, 0.0109, 0.0955, 6677, 0.7518, 37, 7, 5, 1, 59.998, 0.6663, 0.9598, 550.0, 350.0] -> [134.6054, 732.4129]\n",
      "35: [127.1186, 0.0326, 0.2023, 10029, 0.6916, 58, 5, 5, 2, 8.9219, 0.79, 0.7839, 550.0, 350.0] -> [435.8906, 1932.2294]\n",
      "36: [140.1869, 0.0391, 0.2229, 10214, 0.8593, 70, 5, 4, 2, 112.3256, 0.2839, 0.7826, 550.0, 350.0] -> [146.9939, 1507.6749]\n",
      "37: [169.8113, 0.0255, 0.172, 13557, 0.9161, 69, 5, 4, 1, 8.5929, 0.2839, 0.2469, 550.0, 350.0] -> [147.12, 392.7355]\n",
      "38: [116.2791, 0.0121, 0.2253, 2790, 1.6483, 63, 4, 3, 1, 51.6616, 0.9343, 0.2477, 550.0, 350.0] -> [223.2471, 879.385]\n",
      "39: [135.1351, 0.0378, 0.1944, 6473, 0.781, 122, 6, 4, 2, 83.8778, 0.7667, 0.9138, 550.0, 350.0] -> [101.5045, 829.8327]\n",
      "40: [114.7959, 0.0193, 0.1472, 10726, 0.5597, 52, 8, 5, 2, 171.1924, 0.1961, 0.9731, 550.0, 350.0] -> [52.7942, 812.402]\n",
      "41: [138.8889, 0.013, 0.1897, 13770, 0.6243, 14, 6, 7, 2, 92.2308, 0.8142, 0.8705, 550.0, 350.0] -> [221.9869, 2306.58]\n",
      "42: [184.4262, 0.0133, 0.1206, 12656, 0.8213, 170, 5, 3, 1, 25.5385, 0.5115, 0.9999, 550.0, 350.0] -> [43.8154, 627.4901]\n",
      "43: [173.0769, 0.0457, 0.2016, 5464, 0.784, 130, 5, 4, 1, 22.2266, 0.2125, 0.9111, 550.0, 350.0] -> [224.9704, 305.5782]\n",
      "44: [182.9268, 0.0429, 0.1394, 7842, 0.7851, 159, 8, 4, 1, 154.2607, 0.6257, 0.9111, 550.0, 350.0] -> [53.1591, 597.468]\n",
      "45: [127.1186, 0.064, 0.1966, 6766, 1.5252, 73, 6, 4, 1, 29.5757, 0.6312, 0.3742, 550.0, 350.0] -> [257.4438, 150.3251]\n",
      "46: [119.0476, 0.0214, 0.1387, 8967, 0.9583, 93, 5, 3, 1, 97.1944, 0.1037, 0.3663, 550.0, 350.0] -> [315.5311, 219.7976]\n",
      "47: [159.0106, 0.0059, 0.2267, 3745, 0.6357, 128, 6, 5, 2, 25.4111, 0.1048, 0.7492, 550.0, 350.0] -> [316.7472, 861.2394]\n",
      "48: [188.2845, 0.0394, 0.1795, 8759, 1.3367, 47, 5, 5, 1, 45.8261, 0.4944, 0.9, 550.0, 350.0] -> [316.8393, 530.1154]\n",
      "49: [140.1869, 0.0296, 0.5174, 8121, 0.6854, 118, 8, 4, 2, 80.0958, 0.2449, 0.9615, 550.0, 350.0] -> [317.439, 644.2965]\n",
      "50: [167.2862, 0.0202, 0.1536, 10259, 0.927, 132, 8, 4, 1, 86.0634, 0.6273, 0.8602, 550.0, 350.0] -> [115.6247, 1020.5743]\n",
      "51: [160.1423, 0.0215, 0.1547, 7593, 0.6087, 74, 7, 6, 2, 188.4085, 0.1535, 0.2204, 550.0, 350.0] -> [335.4641, 475.4434]\n",
      "52: [160.1423, 0.0107, 0.1133, 2588, 0.8581, 104, 8, 5, 1, 99.3662, 0.2779, 0.731, 550.0, 350.0] -> [348.9426, 714.1963]\n",
      "53: [138.0368, 0.0281, 0.1156, 14615, 0.8568, 103, 5, 4, 2, 69.0679, 0.2809, 0.7829, 550.0, 350.0] -> [59.8091, 1774.3952]\n",
      "54: [166.0517, 0.024, 0.1076, 10859, 0.4955, 94, 6, 3, 1, 94.8361, 0.6411, 0.7838, 550.0, 350.0] -> [35.7718, 1777.6143]\n",
      "55: [122.9508, 0.0216, 0.1274, 7313, 0.6594, 78, 7, 5, 1, 72.2419, 0.5695, 0.7159, 550.0, 350.0] -> [499.9876, 2999.913]\n",
      "56: [120.3209, 0.0214, 0.1174, 6619, 0.6565, 62, 7, 5, 2, 115.327, 0.27, 0.7147, 550.0, 350.0] -> [152.9265, 2998.6086]\n",
      "57: [135.1351, 0.0232, 0.1209, 10485, 0.579, 50, 7, 5, 2, 75.6016, 0.287, 0.8795, 550.0, 350.0] -> [368.7665, 409.3572]\n",
      "58: [109.2233, 0.5251, 0.7008, 11329, 1.5085, 18, 3, 5, 2, 3.8478, 0.2181, 0.8451, 550.0, 350.0] -> [55.3032, 885.4696]\n",
      "59: [154.6392, 0.0136, 0.1331, 1518, 1.0145, 51, 9, 5, 1, 20.0395, 0.569, 0.7956, 550.0, 350.0] -> [153.894, 434.4418]\n"
     ]
    }
   ],
   "source": [
    "# input_data, output_data = taunet_utils.create_dataset(join(\"dataset\", \"audio\"), \n",
    "#                                                       join(\"dataset\", \"human_input\", \"AITD_Dataset_Kristof_beta_1.csv\"),\n",
    "#                                                       join(\"..\", \"src\", \"af\", \"AFInC.dll\"),\n",
    "#                                                       join(\"dataset\", \"saved\", \"AITD_Dataset_Kristof_beta_1.json\"))\n",
    "\n",
    "with open(join(\"dataset\", \"saved\", \"AITD_Dataset_Kristof_beta_1in.json\"), \"r\") as json_file:\n",
    "    input_data = json.load(json_file)\n",
    "with open(join(\"dataset\", \"saved\", \"AITD_Dataset_Kristof_beta_1out.json\"), \"r\") as json_file:\n",
    "    output_data = json.load(json_file)\n",
    "\n",
    "for d in input_data:\n",
    "    print(f\"{d}: {input_data[d]} -> {output_data[d]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                960       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 2)                 10        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,806\n",
      "Trainable params: 4,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(4, activation='relu'))\n",
    "    # model.add(layers.LSTM(64, return_sequences=True))   # short term memory, useful if input data is related accross vectors\n",
    "    model.add(Dense(2, kernel_regularizer=tf.keras.regularizers.l2(0.001))) # no activation (linear): continuous mapping of outputs (this is not a classification task!)\n",
    "    return model\n",
    "\n",
    "input_shape = (14,)\n",
    "model = create_model(input_shape)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 11700), started 3 days, 21:08:59 ago. (Use '!kill 11700' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-90ab8e78b1524274\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-90ab8e78b1524274\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "2/2 [==============================] - 3s 602ms/step - loss: 886319.1875 - accuracy: 0.8800 - val_loss: 680546.8125 - val_accuracy: 0.6667\n",
      "Epoch 2/80\n",
      "2/2 [==============================] - 0s 163ms/step - loss: 836673.6250 - accuracy: 0.8800 - val_loss: 672512.5000 - val_accuracy: 0.6667\n",
      "Epoch 3/80\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 809420.1875 - accuracy: 0.8800 - val_loss: 663762.3125 - val_accuracy: 0.6667\n",
      "Epoch 4/80\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 779153.3750 - accuracy: 0.8800 - val_loss: 656800.6250 - val_accuracy: 0.6667\n",
      "Epoch 5/80\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 794322.1875 - accuracy: 0.9375WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0014s vs `on_train_batch_end` time: 0.0050s). Check your callbacks.\n",
      "2/2 [==============================] - 0s 168ms/step - loss: 751862.6250 - accuracy: 0.8800 - val_loss: 644985.6875 - val_accuracy: 0.6667\n",
      "Epoch 6/80\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 717817.2500 - accuracy: 0.8800 - val_loss: 633204.6875 - val_accuracy: 0.6667\n",
      "Epoch 7/80\n",
      "2/2 [==============================] - 0s 160ms/step - loss: 682483.2500 - accuracy: 0.8800 - val_loss: 622187.1875 - val_accuracy: 0.6667\n",
      "Epoch 8/80\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 651126.7500 - accuracy: 0.8800 - val_loss: 609620.2500 - val_accuracy: 0.6667\n",
      "Epoch 9/80\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 621401.8125 - accuracy: 0.8800 - val_loss: 599518.4375 - val_accuracy: 0.6667\n",
      "Epoch 10/80\n",
      "2/2 [==============================] - 0s 163ms/step - loss: 585008.4375 - accuracy: 0.8800 - val_loss: 588372.5625 - val_accuracy: 0.6667\n",
      "Epoch 11/80\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 559179.1875 - accuracy: 0.8800 - val_loss: 577853.0000 - val_accuracy: 0.6667\n",
      "Epoch 12/80\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 537339.0625 - accuracy: 0.8800 - val_loss: 569875.5000 - val_accuracy: 0.6667\n",
      "Epoch 13/80\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 516746.8125 - accuracy: 0.8800 - val_loss: 561765.4375 - val_accuracy: 0.6667\n",
      "Epoch 14/80\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 499702.4062 - accuracy: 0.8800 - val_loss: 555567.6250 - val_accuracy: 0.6667\n",
      "Epoch 15/80\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 488784.3125 - accuracy: 0.8800 - val_loss: 553484.6875 - val_accuracy: 0.6667\n",
      "Epoch 16/80\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 481365.1250 - accuracy: 0.8800 - val_loss: 549633.5625 - val_accuracy: 0.6667\n",
      "Epoch 17/80\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 475394.4688 - accuracy: 0.8800 - val_loss: 542761.8125 - val_accuracy: 0.6667\n",
      "Epoch 18/80\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 472706.1250 - accuracy: 0.8800 - val_loss: 540928.5000 - val_accuracy: 0.6667\n",
      "Epoch 19/80\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 473345.5625 - accuracy: 0.8800 - val_loss: 539295.7500 - val_accuracy: 0.6667\n",
      "Epoch 20/80\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 472367.4062 - accuracy: 0.8800 - val_loss: 537854.3750 - val_accuracy: 0.6667\n",
      "Epoch 21/80\n",
      "2/2 [==============================] - 0s 160ms/step - loss: 470250.2812 - accuracy: 0.8800 - val_loss: 536538.5625 - val_accuracy: 0.6667\n",
      "Epoch 22/80\n",
      "2/2 [==============================] - 0s 206ms/step - loss: 467793.8125 - accuracy: 0.8800 - val_loss: 535444.1875 - val_accuracy: 0.6667\n",
      "Epoch 23/80\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 463969.5625 - accuracy: 0.8800 - val_loss: 534484.1250 - val_accuracy: 0.6667\n",
      "Epoch 24/80\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 461209.5312 - accuracy: 0.8800 - val_loss: 533655.5625 - val_accuracy: 0.6667\n",
      "Epoch 25/80\n",
      "2/2 [==============================] - 0s 178ms/step - loss: 458552.7188 - accuracy: 0.8800 - val_loss: 532946.1875 - val_accuracy: 0.6667\n",
      "Epoch 26/80\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 459031.1875 - accuracy: 0.8800 - val_loss: 532410.0625 - val_accuracy: 0.6667\n",
      "Epoch 27/80\n",
      "2/2 [==============================] - 0s 165ms/step - loss: 455454.4062 - accuracy: 0.8800 - val_loss: 531157.6875 - val_accuracy: 0.6667\n",
      "Epoch 28/80\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 453746.8750 - accuracy: 0.8800 - val_loss: 529967.2500 - val_accuracy: 0.6667\n",
      "Epoch 29/80\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 452198.9062 - accuracy: 0.8800 - val_loss: 528679.6875 - val_accuracy: 0.6667\n",
      "Epoch 30/80\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 450290.7500 - accuracy: 0.8800 - val_loss: 527024.2500 - val_accuracy: 0.6667\n",
      "Epoch 31/80\n",
      "2/2 [==============================] - 0s 160ms/step - loss: 448345.1562 - accuracy: 0.8800 - val_loss: 525322.2500 - val_accuracy: 0.6667\n",
      "Epoch 32/80\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 446653.6875 - accuracy: 0.8800 - val_loss: 523424.7188 - val_accuracy: 0.6667\n",
      "Epoch 33/80\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 446595.0938 - accuracy: 0.8800 - val_loss: 521705.2500 - val_accuracy: 0.6667\n",
      "Epoch 34/80\n",
      "2/2 [==============================] - 0s 165ms/step - loss: 444628.9062 - accuracy: 0.8800 - val_loss: 520521.2812 - val_accuracy: 0.6667\n",
      "Epoch 35/80\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 442366.4062 - accuracy: 0.8800 - val_loss: 519922.4688 - val_accuracy: 0.6667\n",
      "Epoch 36/80\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 440303.0312 - accuracy: 0.8800 - val_loss: 519147.8750 - val_accuracy: 0.6667\n",
      "Epoch 37/80\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 439286.6875 - accuracy: 0.8800 - val_loss: 518531.1562 - val_accuracy: 0.6667\n",
      "Epoch 38/80\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 437804.3125 - accuracy: 0.8800 - val_loss: 517520.5312 - val_accuracy: 0.6667\n",
      "Epoch 39/80\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 437029.6875 - accuracy: 0.8800 - val_loss: 516619.0312 - val_accuracy: 0.6667\n",
      "Epoch 40/80\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 435671.5938 - accuracy: 0.8800 - val_loss: 514977.0000 - val_accuracy: 0.6667\n",
      "Epoch 41/80\n",
      "2/2 [==============================] - 0s 168ms/step - loss: 433617.5312 - accuracy: 0.8800 - val_loss: 513602.7812 - val_accuracy: 0.6667\n",
      "Epoch 42/80\n",
      "2/2 [==============================] - 0s 164ms/step - loss: 431788.3125 - accuracy: 0.8800 - val_loss: 511781.1562 - val_accuracy: 0.6667\n",
      "Epoch 43/80\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 430990.4375 - accuracy: 0.8800 - val_loss: 509728.1250 - val_accuracy: 0.6667\n",
      "Epoch 44/80\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 429061.9062 - accuracy: 0.8800 - val_loss: 508164.2812 - val_accuracy: 0.6667\n",
      "Epoch 45/80\n",
      "2/2 [==============================] - 0s 163ms/step - loss: 427782.8438 - accuracy: 0.8800 - val_loss: 506569.7188 - val_accuracy: 0.6667\n",
      "Epoch 46/80\n",
      "2/2 [==============================] - 0s 160ms/step - loss: 426637.8438 - accuracy: 0.8800 - val_loss: 505135.1562 - val_accuracy: 0.6667\n",
      "Epoch 47/80\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 425356.7188 - accuracy: 0.8800 - val_loss: 503971.5938 - val_accuracy: 0.6667\n",
      "Epoch 48/80\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 424359.1875 - accuracy: 0.8800 - val_loss: 503176.7500 - val_accuracy: 0.6667\n",
      "Epoch 49/80\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 421744.5625 - accuracy: 0.8800 - val_loss: 503219.6250 - val_accuracy: 0.6667\n",
      "Epoch 50/80\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 420403.5938 - accuracy: 0.8800 - val_loss: 503189.1562 - val_accuracy: 0.6667\n",
      "Epoch 51/80\n",
      "2/2 [==============================] - 0s 172ms/step - loss: 420063.7188 - accuracy: 0.8800 - val_loss: 503101.0312 - val_accuracy: 0.6667\n",
      "Epoch 52/80\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 419880.6250 - accuracy: 0.8800 - val_loss: 502535.0000 - val_accuracy: 0.6667\n",
      "Epoch 53/80\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 420016.9688 - accuracy: 0.8800 - val_loss: 501445.3750 - val_accuracy: 0.6667\n",
      "Epoch 54/80\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 418041.4375 - accuracy: 0.8800 - val_loss: 499070.4688 - val_accuracy: 0.6667\n",
      "Epoch 55/80\n",
      "2/2 [==============================] - 0s 161ms/step - loss: 415850.1562 - accuracy: 0.8800 - val_loss: 496408.5938 - val_accuracy: 0.6667\n",
      "Epoch 56/80\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 413163.6875 - accuracy: 0.8800 - val_loss: 494036.4062 - val_accuracy: 0.6667\n",
      "Epoch 57/80\n",
      "2/2 [==============================] - 0s 161ms/step - loss: 413260.3125 - accuracy: 0.8800 - val_loss: 491880.4062 - val_accuracy: 0.6667\n",
      "Epoch 58/80\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 412161.0000 - accuracy: 0.8800 - val_loss: 490401.0938 - val_accuracy: 0.6667\n",
      "Epoch 59/80\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 410370.9688 - accuracy: 0.8800 - val_loss: 489504.2812 - val_accuracy: 0.6667\n",
      "Epoch 60/80\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 408671.0312 - accuracy: 0.8800 - val_loss: 488513.8438 - val_accuracy: 0.6667\n",
      "Epoch 61/80\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 407334.1250 - accuracy: 0.8800 - val_loss: 487783.9062 - val_accuracy: 0.6667\n",
      "Epoch 62/80\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 406017.4375 - accuracy: 0.8800 - val_loss: 486881.3438 - val_accuracy: 0.6667\n",
      "Epoch 63/80\n",
      "2/2 [==============================] - 0s 161ms/step - loss: 404825.7500 - accuracy: 0.8800 - val_loss: 485884.6562 - val_accuracy: 0.6667\n",
      "Epoch 64/80\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 403738.0000 - accuracy: 0.8800 - val_loss: 484866.5000 - val_accuracy: 0.6667\n",
      "Epoch 65/80\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 402872.8750 - accuracy: 0.8800 - val_loss: 483738.3438 - val_accuracy: 0.6667\n",
      "Epoch 66/80\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 401689.5312 - accuracy: 0.8800 - val_loss: 482187.0938 - val_accuracy: 0.6667\n",
      "Epoch 67/80\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 400345.3750 - accuracy: 0.8800 - val_loss: 480314.4688 - val_accuracy: 0.6667\n",
      "Epoch 68/80\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 399155.5938 - accuracy: 0.8800 - val_loss: 478336.9062 - val_accuracy: 0.6667\n",
      "Epoch 69/80\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 397679.9062 - accuracy: 0.8800 - val_loss: 476616.3438 - val_accuracy: 0.6667\n",
      "Epoch 70/80\n",
      "2/2 [==============================] - 0s 168ms/step - loss: 397495.0312 - accuracy: 0.8800 - val_loss: 474776.6562 - val_accuracy: 0.6667\n",
      "Epoch 71/80\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 396214.4062 - accuracy: 0.8800 - val_loss: 473508.7500 - val_accuracy: 0.6667\n",
      "Epoch 72/80\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 394884.2500 - accuracy: 0.8800 - val_loss: 471977.7812 - val_accuracy: 0.6667\n",
      "Epoch 73/80\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 393127.6875 - accuracy: 0.8800 - val_loss: 470149.9688 - val_accuracy: 0.6667\n",
      "Epoch 74/80\n",
      "2/2 [==============================] - 0s 191ms/step - loss: 396778.2500 - accuracy: 0.8800 - val_loss: 468341.5938 - val_accuracy: 0.6667\n",
      "Epoch 75/80\n",
      "2/2 [==============================] - 0s 165ms/step - loss: 391730.8125 - accuracy: 0.8800 - val_loss: 467547.3750 - val_accuracy: 0.6667\n",
      "Epoch 76/80\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 391621.7188 - accuracy: 0.8800 - val_loss: 466802.3438 - val_accuracy: 0.6667\n",
      "Epoch 77/80\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 388511.0938 - accuracy: 0.8800 - val_loss: 465325.2500 - val_accuracy: 0.6667\n",
      "Epoch 78/80\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 387444.7188 - accuracy: 0.8800 - val_loss: 464018.2188 - val_accuracy: 0.6667\n",
      "Epoch 79/80\n",
      "2/2 [==============================] - 0s 165ms/step - loss: 386664.8750 - accuracy: 0.8800 - val_loss: 462353.7812 - val_accuracy: 0.6667\n",
      "Epoch 80/80\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 385032.0312 - accuracy: 0.8800 - val_loss: 460909.0938 - val_accuracy: 0.6667\n"
     ]
    }
   ],
   "source": [
    "combined_data = [(input_data[key], output_data[key]) for key in input_data.keys()]\n",
    "\n",
    "train_data, test_data = train_test_split(combined_data, test_size=0.1, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.05, random_state=42)\n",
    "\n",
    "log_dir = \"logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "class LogRMSECallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, log_dir):\n",
    "        super(LogRMSECallback, self).__init__()\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        mse = logs.get('val_loss')\n",
    "        rmse = np.sqrt(mse)\n",
    "        with tf.summary.create_file_writer(self.log_dir).as_default():\n",
    "            tf.summary.scalar('val_rmse', rmse, step=epoch)\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/ --port 6006\n",
    "\n",
    "history = model.fit(\n",
    "    x=np.array([item[0] for item in train_data]),\n",
    "    y=np.array([item[1] for item in train_data]),\n",
    "    validation_data=(\n",
    "        np.array([item[0] for item in val_data]),\n",
    "        np.array([item[1] for item in val_data])\n",
    "    ),\n",
    "    epochs=80,\n",
    "    # callbacks=[tensorboard_callback, early_stopping_callback],\n",
    "    callbacks=[tensorboard_callback, early_stopping_callback, LogRMSECallback(join(log_dir, \"rmse\"))],\n",
    "    # batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 135ms/step\n",
      "Input: [123.9669, 0.012, 0.2251, 15648, 0.9176, 82, 10, 5, 1, 47.5291, 0.6738, 0.8758, 550.0, 350.0]\n",
      "Expected Output: [360.5123, 1144.7427]\n",
      "Predicted Output: [[ -46.935417 1496.0387  ]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Input: [173.7452, 0.0087, 0.1529, 25353, 0.6351, 3, 3, 4, 3, 188.9978, 0.2, 0.9179, 550.0, 350.0]\n",
      "Expected Output: [91.6004, 2420.8156]\n",
      "Predicted Output: [[ -74.492905 2371.4602  ]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Input: [127.1186, 0.0326, 0.2023, 10029, 0.6916, 58, 5, 5, 2, 8.9219, 0.79, 0.7839, 550.0, 350.0]\n",
      "Expected Output: [435.8906, 1932.2294]\n",
      "Predicted Output: [[-31.22067 996.8269 ]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Input: [152.5424, 0.0422, 0.245, 2114, 1.2588, 24, 6, 6, 1, 18.9737, 0.6313, 0.2773, 550.0, 350.0]\n",
      "Expected Output: [281.9786, 461.7476]\n",
      "Predicted Output: [[ -9.0678835 293.09726  ]]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Input: [119.0476, 0.0214, 0.1387, 8967, 0.9583, 93, 5, 3, 1, 97.1944, 0.1037, 0.3663, 550.0, 350.0]\n",
      "Expected Output: [315.5311, 219.7976]\n",
      "Predicted Output: [[-28.625414 914.3832  ]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Input: [166.0517, 0.024, 0.1076, 10859, 0.4955, 94, 6, 3, 1, 94.8361, 0.6411, 0.7838, 550.0, 350.0]\n",
      "Expected Output: [35.7718, 1777.6143]\n",
      "Predicted Output: [[ -34.01207 1085.5016 ]]\n",
      "{'Train loss': 384188.25, 'Train accuracy': 0.8799999952316284}\n",
      "{'Validation loss': 460909.09375, 'Validation accuracy': 0.6666666865348816}\n",
      "{'Test loss': 217545.796875, 'Test accuracy': 0.8333333134651184}\n",
      "RMSE for test set: 466.41804947385987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/20240429-125746\\taunet.pb\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/20240429-125746\\taunet.pb\\assets\n"
     ]
    }
   ],
   "source": [
    "train_metrics = model.evaluate(\n",
    "    np.array([item[0] for item in train_data]),\n",
    "    np.array([item[1] for item in train_data]),\n",
    "    verbose=0\n",
    ")\n",
    "val_metrics = model.evaluate(\n",
    "    np.array([item[0] for item in val_data]),\n",
    "    np.array([item[1] for item in val_data]),\n",
    "    verbose=0\n",
    ")\n",
    "test_metrics = model.evaluate(\n",
    "    np.array([item[0] for item in test_data]),\n",
    "    np.array([item[1] for item in test_data]),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "for item in test_data:\n",
    "    inp = np.array([item[0]])  # Reshape input data into a batch\n",
    "    prediction = model.predict(inp)\n",
    "    print(\"Input:\", item[0])\n",
    "    print(\"Expected Output:\", item[1])\n",
    "    print(\"Predicted Output:\", prediction)\n",
    "\n",
    "\n",
    "print({f\"Train {metric_name}\": metric_value for metric_name, metric_value in zip(model.metrics_names, train_metrics)})\n",
    "print({f\"Validation {metric_name}\": metric_value for metric_name, metric_value in zip(model.metrics_names, val_metrics)})\n",
    "print({f\"Test {metric_name}\": metric_value for metric_name, metric_value in zip(model.metrics_names, test_metrics)})\n",
    "print(\"RMSE for test set:\", np.sqrt(test_metrics[0]))\n",
    "\n",
    "# save model as .h5, .tflite and .json\n",
    "model.save(join(log_dir, \"taunet.pb\"))\n",
    "tflite_model = tf.lite.TFLiteConverter.from_saved_model(join(log_dir, \"taunet.pb\")).convert()\n",
    "with open(join(log_dir, \"taunet.tflite\"), 'wb') as f:\n",
    "  f.write(tflite_model)\n",
    "save_model(model, join(log_dir, \"taunet.json\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
